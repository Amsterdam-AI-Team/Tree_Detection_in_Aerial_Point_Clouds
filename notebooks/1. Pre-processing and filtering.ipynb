{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344c6f6a-6a65-4fd5-901f-ad233350078b",
   "metadata": {},
   "source": [
    "# Pre-processing and Filtering\n",
    "\n",
    "This notebook takes as input a folder with AHN point cloud tiles (assumed to be 1x1 km), and performs a number of pre-processing and filtering steps. The output is a reduced point cloud containing trees and \"suspected trees\" for further classification using RandLA-Net and / or post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba0cd-6204-4b97-8739-fa01dd93bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_path\n",
    "\n",
    "import numpy as np\n",
    "import laspy\n",
    "import pathlib\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as sg\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from upcp.utils import ahn_utils\n",
    "from upcp.utils.interpolation import FastGridInterpolator\n",
    "from upcp.region_growing.label_connected_comp import LabelConnectedComp\n",
    "\n",
    "import gvl.helper_functions as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f19ce-ca70-4952-9db2-5f8bdf8130fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  # temporary, to supress deprecationwarnings from shapely\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de515d-1ad2-48a9-afd7-7b6e09eb093e",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d5655-b6b9-4e74-bdee-27d8914c9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = pathlib.Path('../datasets/AHN4')\n",
    "\n",
    "# Input: AHN subtiles created using notebook \"0. LAS Splitter.ipynb\"\n",
    "input_dir = BASE_FOLDER / 'AMS_subtiles_1000'\n",
    "output_dir = BASE_FOLDER / 'AMS_subtiles_1000_reduced'\n",
    "\n",
    "# DTM corresponding to AHN subtiles, stored as .npz, created using notebook \"0. LAS Splitter.ipynb\"\n",
    "ahn_dtm_folder = BASE_FOLDER / 'npz_subtiles_1000'\n",
    "\n",
    "# Known tree locations, if available.\n",
    "tree_ref_file = '../datasets/validation/joined_trees_bgt_gissib_1_5_amsterdam.gpkg' # TODO get geovisia dataset\n",
    "\n",
    "MIN_HAG = 2.5  # Minimum height above ground in meters\n",
    "\n",
    "tree_filter = {'grid_size': 0.6,\n",
    "               'min_component_size': 50,\n",
    "               'min_height': 3.5}  # TODO\n",
    "noise_filter = {'grid_size': 0.9,\n",
    "                'min_component_size': 50}  # TODO\n",
    "other_filter = {'min_nz_flat': 0.85,\n",
    "                'min_width': 1.0,\n",
    "                'min_height': 2.5,\n",
    "                'max_height': 38}  # TODO\n",
    "\n",
    "# AHN classification\n",
    "AHN_OTHER = 1\n",
    "AHN_GROUND = 2\n",
    "AHN_BUILDING = 6\n",
    "AHN_WATER = 9\n",
    "AHN_ARTIFACT = 26\n",
    "\n",
    "# Our classification\n",
    "UNKNOWN = 0\n",
    "TREE = 1\n",
    "NOISE = 2\n",
    "OTHER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d352d-5176-492f-80f4-696e417d8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_gdf = gpd.read_file(tree_ref_file, crs='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d265f-474d-4191-bc98-290b2cc1ada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder\n",
    "pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create DTM reader\n",
    "ahn_reader = ahn_utils.NPZReader(ahn_dtm_folder, caching=False)\n",
    "\n",
    "# Load tree locations\n",
    "tree_gdf = gpd.read_file(tree_ref_file, crs='epsg:28992')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8364d-c398-46dc-8b19-424f5d6e316c",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89643a0-b975-41f3-959b-cb2c10bae8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = list(pathlib.Path(input_dir).glob('ahn4*.laz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34893c1c-6af3-423c-a62e-edfb1de33846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing output files (ignore this cell to re-run for all tiles)\n",
    "existing_files = list(pathlib.Path(output_dir).glob('tree*.laz'))\n",
    "existing_codes = {helpers.get_tilecode_from_filename(file.name)\n",
    "                  for file in existing_files}\n",
    "\n",
    "input_files = [file for file in input_files\n",
    "               if helpers.get_tilecode_from_filename(file.name) not in existing_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827d96e-bd28-4ab4-9476-d922b5fa7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(input_files, unit='file', smoothing=0)\n",
    "\n",
    "for file in pbar:\n",
    "    tilecode = helpers.get_tilecode_from_filename(file.name)\n",
    "    pbar.set_postfix_str(tilecode)\n",
    "    \n",
    "    # Load LAS data\n",
    "    las = laspy.read(file)\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).T\n",
    "    \n",
    "    # Get trees inside LAS bounding box\n",
    "    bbox = sg.box(las.header.min[0], las.header.min[1], las.header.max[0], las.header.max[1], ccw=True)\n",
    "    trees_in_bbox = tree_gdf[tree_gdf.within(bbox)]\n",
    "    tree_points = list(trees_in_bbox['geometry'].values)\n",
    "    \n",
    "    \n",
    "    ### Reduce the point cloud\n",
    "    \n",
    "    # Check if las file is valid\n",
    "    if np.all(las.classification==0):\n",
    "        print(f'No classification provided in the laz file {file.name}. Aborting...')\n",
    "        break\n",
    "    \n",
    "    las.classification\n",
    "    # Use only AHN_OTHER class\n",
    "    # We dont want to filter on the number_of_returns scalar field. It will remove too much valuable tree points.\n",
    "    mask = (las.classification == AHN_OTHER)\n",
    "\n",
    "    # Remove points close to ground\n",
    "    ground_z = ahn_reader.interpolate(tilecode,\n",
    "                                      points_xyz[mask])\n",
    "    height_mask = (points_xyz[mask, 2] - ground_z >= MIN_HAG) | np.isnan(ground_z)\n",
    "\n",
    "    points_xyz = points_xyz[mask][height_mask]\n",
    "    points_i = las.intensity[mask][height_mask]\n",
    "    ground_z = ground_z[height_mask]\n",
    "    points_orig_idx = np.where(mask)[0][height_mask]\n",
    "\n",
    "\n",
    "    ### Generate two new scalar fields\n",
    "\n",
    "    # Compute normals\n",
    "    normals = helpers.calculate_normals(points_xyz)\n",
    "\n",
    "    # Compute height above ground\n",
    "    hag = points_xyz[:,2] - ground_z\n",
    "\n",
    "    \n",
    "    ### Label the point cloud\n",
    "    \n",
    "    # Init masks\n",
    "    normals_mask = np.zeros(len(points_xyz), dtype=bool)\n",
    "    dims_mask = np.zeros(len(points_xyz), dtype=bool)\n",
    "    hag_mask = np.zeros(len(points_xyz), dtype=bool)    \n",
    "    new_hag = np.copy(hag) # Because we index and manipulate this array in a for loop\n",
    "\n",
    "    ## Label \"tree\" clusters based on ground truth tree points\n",
    "    lcc = LabelConnectedComp(grid_size=tree_filter['grid_size'],\n",
    "                             min_component_size=tree_filter['min_component_size'])\n",
    "    point_components = lcc.get_components(points_xyz)\n",
    "\n",
    "    tree_mask = helpers.label_tree_like_components(\n",
    "                                    points_xyz, ground_z, point_components,\n",
    "                                    tree_points, tree_filter['min_height'])\n",
    "    \n",
    "    cc_labels = np.unique(point_components)\n",
    "    cc_labels = set(cc_labels).difference((-1,))\n",
    "\n",
    "    # Iterate over the clusters\n",
    "    for cc in tqdm(cc_labels, smoothing=0, leave=False):\n",
    "        # select points that belong to the cluster\n",
    "        cc_mask = (point_components == cc)\n",
    "        cc_z = points_xyz[cc_mask][:, 2]\n",
    "        cc_hag = hag[cc_mask]\n",
    "        \n",
    "        if np.isnan(cc_hag).all():\n",
    "            cc_offset = 0.\n",
    "        else:\n",
    "            cc_offset = np.nanmean(cc_hag) - np.mean(cc_z)\n",
    "        new_hag[cc_mask] = cc_z + cc_offset\n",
    "\n",
    "    ## Label \"noise\" points\n",
    "    mask_ids = np.where(~tree_mask)[0] # Only the possible noise points\n",
    "    lcc = LabelConnectedComp(grid_size=noise_filter['grid_size'],\n",
    "                             min_component_size=noise_filter['min_component_size'])\n",
    "    point_components = lcc.get_components(points_xyz[mask_ids])\n",
    "\n",
    "    noise_mask = np.zeros((len(points_xyz),), dtype=bool)\n",
    "    noise_mask[mask_ids] = point_components == -1\n",
    "\n",
    "    ## Label \"non-tree\" clusters based on normal values and HAG\n",
    "    mask_ids = np.where(~(tree_mask | noise_mask))[0]\n",
    "    lcc = LabelConnectedComp(grid_size=tree_filter['grid_size'],\n",
    "                             min_component_size=tree_filter['min_component_size'])\n",
    "    point_components = lcc.get_components(points_xyz[mask_ids])\n",
    "    \n",
    "    cc_labels = np.unique(point_components)\n",
    "    cc_labels = set(cc_labels).difference((-1,))\n",
    "\n",
    "    # Iterate over the clusters\n",
    "    for cc in cc_labels:\n",
    "        # select points that belong to the cluster\n",
    "        cc_mask = (point_components == cc)\n",
    "\n",
    "        # If most of the points point up, it's not a tree.\n",
    "        if np.abs(normals[:,2][mask_ids[cc_mask]]).mean() > other_filter['min_nz_flat']:\n",
    "            normals_mask[mask_ids[cc_mask]] = True\n",
    "\n",
    "        # TODO come up with something clever for x/y flatness\n",
    "        # Do a similar thing with the x normals\n",
    "        # if normals[:,0][mask_ids[cc_mask]].mean() < 0.03 and normals[:,0][mask_ids[cc_mask]].mean() > -0.03:\n",
    "        #     normals_mask[mask_ids[cc_mask]] = True\n",
    "\n",
    "        # Look at shape of cluster, e.g. minimum bounding rectangle + min_width check.\n",
    "        min_dim, _ = helpers.get_wl_box(points_xyz[mask_ids[cc_mask]])\n",
    "        if min_dim < other_filter['min_width']:\n",
    "            dims_mask[mask_ids[cc_mask]] = True\n",
    "\n",
    "        # If the object is smaller than MIN_HEIGHT or higher than MAX_HEIGHT, it's not a tree.\n",
    "        if new_hag[mask_ids[cc_mask]].max() > other_filter['max_height'] or new_hag[mask_ids[cc_mask]].max() < other_filter['min_height']:\n",
    "            hag_mask[mask_ids[cc_mask]] = True\n",
    "\n",
    "    ## Set labels\n",
    "    labels = np.ones((len(points_xyz),), dtype='uint16') * UNKNOWN\n",
    "    labels[tree_mask] = TREE\n",
    "    labels[noise_mask] = NOISE\n",
    "    labels[normals_mask] = OTHER\n",
    "    labels[dims_mask] = OTHER\n",
    "    labels[hag_mask] = OTHER\n",
    "\n",
    "    \n",
    "    ### Save the point cloud\n",
    "\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.2\")\n",
    "    header.offsets = las.header.offsets\n",
    "    header.scales = las.header.scales\n",
    "\n",
    "    new_las = laspy.LasData(header)\n",
    "\n",
    "    new_las.x = points_xyz[:, 0]\n",
    "    new_las.y = points_xyz[:, 1]\n",
    "    new_las.z = points_xyz[:, 2]\n",
    "    new_las.intensity = points_i\n",
    "\n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"label\", type=\"uint16\",\n",
    "                                                 description=\"Label\"))  \n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"orig_idx\", type=\"uint32\",\n",
    "                                                 description=\"Original index\"))  \n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"hag\", type=\"float\",\n",
    "                                                 description=\"Height above ground\"))\n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"normal_x\", type=\"float\",\n",
    "                                                 description=\"normal_x\"))\n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"normal_y\", type=\"float\",\n",
    "                                                 description=\"normal_y\"))    \n",
    "    new_las.add_extra_dim(laspy.ExtraBytesParams(name=\"normal_z\", type=\"float\",\n",
    "                                                 description=\"normal_z\"))\n",
    "    \n",
    "    new_las.label = labels\n",
    "    new_las.orig_idx = points_orig_idx\n",
    "    new_las.hag = new_hag\n",
    "    new_las.normal_x = normals[:,0]\n",
    "    new_las.normal_y = normals[:,1]\n",
    "    new_las.normal_z = normals[:,2]\n",
    "\n",
    "    new_las.write(output_dir / f'tree_{tilecode}.laz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091d643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
